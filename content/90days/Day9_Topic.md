<div style="text-align: center;">
  <h1>ğŸš€ Becoming a Scikit-Learn Boss in 90 Days: Day 9 â€“ Time Series Analysis ğŸ“…ğŸ“ˆ</h1>
  <p>Master the Techniques to Analyze and Forecast Time-Dependent Data!</p>
</div>

---

## ğŸ“‘ Table of Contents

1. [ğŸŒŸ Welcome to Day 9](#welcome-to-day-9)
2. [ğŸ” Review of Day 8 ğŸ“œ](#review-of-day-8-ğŸ“œ)
3. [ğŸ§  Introduction to Time Series Analysis ğŸ§ ](#introduction-to-time-series-analysis-ğŸ§ )
    - [ğŸ“š What is Time Series Analysis?](#what-is-time-series-analysis-ğŸ“š)
    - [ğŸ” Importance of Time Series Analysis](#importance-of-time-series-analysis-ğŸ”)
4. [ğŸ“Š Time Series Forecasting Techniques ğŸ“Š](#time-series-forecasting-techniques-ğŸ“Š)
    - [ğŸ”„ Moving Averages](#moving-averages-ğŸ”„)
    - [ğŸ“ˆ Exponential Smoothing](#exponential-smoothing-ğŸ“ˆ)
    - [ğŸ“‰ Autoregressive (AR) Models](#autoregressive-ar-models-ğŸ“‰)
    - [ğŸ”— Autoregressive Integrated Moving Average (ARIMA)](#autoregressive-integrated-moving-average-arima-ğŸ”—)
    - [ğŸ§° Machine Learning Approaches](#machine-learning-approaches-ğŸ§°)
        - [ğŸ“ Regression-Based Models](#regression-based-models-ğŸ“)
        - [ğŸŒŸ Support Vector Regression (SVR)](#support-vector-regression-svr-ğŸŒŸ)
        - [ğŸ› ï¸ Random Forest Regression](#random-forest-regression-ğŸ› ï¸)
5. [ğŸ› ï¸ Implementing Time Series Analysis with Scikit-Learn ğŸ› ï¸](#implementing-time-series-analysis-with-scikit-learn-ğŸ› ï¸)
    - [ğŸ”„ Preparing Time Series Data](#preparing-time-series-data-ğŸ”„)
    - [ğŸ“ Feature Engineering for Time Series](#feature-engineering-for-time-series-ğŸ“)
    - [ğŸ“ˆ Forecasting with Regression Models](#forecasting-with-regression-models-ğŸ“ˆ)
    - [ğŸŒŸ Support Vector Regression Example ğŸŒŸ](#support-vector-regression-example-ğŸŒŸ)
    - [ğŸ› ï¸ Random Forest Regression Example ğŸ› ï¸](#random-forest-regression-example-ğŸ› ï¸)
6. [ğŸ“ˆ Model Evaluation for Time Series ğŸ“ˆ](#model-evaluation-for-time-series-ğŸ“ˆ)
    - [ğŸ“‰ Mean Absolute Error (MAE) ğŸ“‰]
    - [ğŸ“ Mean Squared Error (MSE) ğŸ“]
    - [ğŸ“ Root Mean Squared Error (RMSE) ğŸ“]
    - [ğŸ“ˆ Mean Absolute Percentage Error (MAPE) ğŸ“ˆ]
    - [ğŸ”„ Cross-Validation for Time Series](#cross-validation-for-time-series)
7. [ğŸ› ï¸ğŸ“ˆ Example Project: Sales Forecasting ğŸ› ï¸ğŸ“ˆ](#example-project-sales-forecasting-ğŸ› ï¸ğŸ“ˆ)
    - [ğŸ“‹ Project Overview](#project-overview-ğŸ“‹)
    - [ğŸ“ Step-by-Step Guide](#step-by-step-guide-ğŸ“)
        - [1. Load and Explore the Dataset](#1-load-and-explore-the-dataset)
        - [2. Data Preprocessing](#2-data-preprocessing)
        - [3. Feature Engineering](#3-feature-engineering)
        - [4. Building Forecasting Models](#4-building-forecasting-models)
        - [5. Evaluating Model Performance](#5-evaluating-model-performance)
        - [6. Selecting the Best Model](#6-selecting-the-best-model)
        - [7. Making Future Predictions](#7-making-future-predictions)
    - [ğŸ“Š Results and Insights](#results-and-insights-ğŸ“Š)
8. [ğŸš€ğŸ“ Conclusion and Next Steps ğŸš€ğŸ“](#conclusion-and-next-steps-ğŸš€ğŸ“)
9. [ğŸ“œ Summary of Day 9 ğŸ“œ](#summary-of-day-9-ğŸ“œ)

---

## 1. ğŸŒŸ Welcome to Day 9

Welcome to **Day 9** of "Becoming a Scikit-Learn Boss in 90 Days"! Today, we'll delve into **Time Series Analysis**, a crucial area for analyzing and forecasting data points collected or recorded at specific time intervals. You'll learn about various time series forecasting techniques, implement them using Scikit-Learn, and apply these methods to real-world datasets to make accurate predictions.

---

## 2. ğŸ” Review of Day 8 ğŸ“œ

Before diving into today's topics, let's briefly recap what we covered yesterday:

- **Model Deployment**: Learned how to serialize machine learning models using Joblib and Pickle, create RESTful APIs with Flask and FastAPI, and deploy models using advanced tools like MLflow.
- **Steps to Deploy a Model**: Covered the entire deployment pipeline from training and serialization to setting up APIs and monitoring.
- **Example Project**: Successfully deployed a Random Forest Regressor model as a Flask API to predict housing prices, demonstrating real-world applicability.

With this foundation, we're ready to explore the fascinating world of time series analysis, enhancing our ability to work with sequential data and make informed forecasts.

---

## 3. ğŸ§  Introduction to Time Series Analysis ğŸ§ 

### ğŸ“š What is Time Series Analysis? ğŸ“š

**Time Series Analysis** involves statistical techniques to model and predict future values based on previously observed values. It is widely used in various domains such as finance, economics, weather forecasting, sales forecasting, and more.

**Key Characteristics of Time Series Data:**

- **Temporal Ordering**: Data points are ordered in time.
- **Seasonality**: Patterns that repeat at regular intervals (e.g., monthly sales).
- **Trend**: Long-term increase or decrease in the data.
- **Stationarity**: Statistical properties like mean and variance remain constant over time.

### ğŸ” Importance of Time Series Analysis ğŸ”

- **Forecasting**: Predict future values (e.g., stock prices, demand forecasting).
- **Anomaly Detection**: Identify unusual patterns or outliers.
- **Seasonal Adjustment**: Remove seasonal effects to better understand underlying trends.
- **Economic Planning**: Aid in making informed business and policy decisions.

---

## 4. ğŸ“Š Time Series Forecasting Techniques ğŸ“Š

### ğŸ”„ Moving Averages ğŸ”„

A technique to smooth out short-term fluctuations and highlight longer-term trends or cycles.

**Types:**

- **Simple Moving Average (SMA)**: Calculates the average of a fixed number of past observations.
- **Weighted Moving Average (WMA)**: Assigns different weights to past observations, giving more importance to recent data.

### ğŸ“ˆ Exponential Smoothing ğŸ“ˆ

Applies exponentially decreasing weights to past observations, giving more importance to recent data points.

**Types:**

- **Single Exponential Smoothing**: Suitable for data with no trend or seasonality.
- **Double Exponential Smoothing**: Accounts for trends in the data.
- **Triple Exponential Smoothing (Holt-Winters)**: Handles both trend and seasonality.

### ğŸ“‰ Autoregressive (AR) Models ğŸ“‰

Models that use the dependent relationship between an observation and a number of lagged observations.

**AR(p)**: Autoregressive model of order p, where p is the number of lagged observations.

### ğŸ”— Autoregressive Integrated Moving Average (ARIMA) ğŸ”—

Combines autoregressive and moving average components, with differencing to make the time series stationary.

**Components:**

- **AR (p)**: Autoregressive part.
- **I (d)**: Differencing to achieve stationarity.
- **MA (q)**: Moving average part.

### ğŸ§° Machine Learning Approaches ğŸ§°

#### ğŸ“ Regression-Based Models ğŸ“

Transform time series forecasting into a regression problem by using lagged values and other time-based features as predictors.

#### ğŸŒŸ Support Vector Regression (SVR) ğŸŒŸ

Uses Support Vector Machines for regression tasks, capable of modeling non-linear relationships.

#### ğŸ› ï¸ Random Forest Regression ğŸ› ï¸

An ensemble method that builds multiple decision trees and averages their predictions, robust to overfitting.

---

## 5. ğŸ› ï¸ Implementing Time Series Analysis with Scikit-Learn ğŸ› ï¸

### ğŸ”„ Preparing Time Series Data ğŸ”„

1. **Stationarity Check**: Use tests like Augmented Dickey-Fuller (ADF) to check for stationarity.
2. **Differencing**: Apply differencing to remove trends and make the series stationary.
3. **Lag Features**: Create lagged features to capture temporal dependencies.

```python
import pandas as pd
import numpy as np
from statsmodels.tsa.stattools import adfuller

# Sample Time Series Data
data = {
    'Date': pd.date_range(start='2020-01-01', periods=100, freq='D'),
    'Value': np.random.randn(100).cumsum() + 50
}
df = pd.DataFrame(data)
df.set_index('Date', inplace=True)

# Plot the Time Series
df['Value'].plot(title='Sample Time Series')
plt.show()

# Augmented Dickey-Fuller Test
result = adfuller(df['Value'])
print(f'ADF Statistic: {result[0]}')
print(f'p-value: {result[1]}')
```

### ğŸ“ Feature Engineering for Time Series ğŸ“

Create features such as lagged values, rolling statistics, and time-based features.

```python
# Create Lag Features
df['Lag1'] = df['Value'].shift(1)
df['Lag2'] = df['Value'].shift(2)

# Create Rolling Mean
df['Rolling_Mean_3'] = df['Value'].rolling(window=3).mean()

# Create Time-Based Features
df['Day'] = df.index.day
df['Month'] = df.index.month
df['Weekday'] = df.index.weekday

# Drop NaN Values
df.dropna(inplace=True)
print(df.head())
```

### ğŸ“ˆ Forecasting with Regression Models ğŸ“ˆ

Use regression models by treating the problem as predicting the next value based on previous features.

```python
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# Define Features and Target
X = df[['Lag1', 'Lag2', 'Rolling_Mean_3', 'Day', 'Month', 'Weekday']]
y = df['Value']

# Split the Data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)

# Initialize and Train the Model
model = LinearRegression()
model.fit(X_train, y_train)

# Make Predictions
y_pred = model.predict(X_test)

# Evaluate the Model
mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error: {mse:.2f}")
```

### ğŸŒŸ Support Vector Regression Example ğŸŒŸ

```python
from sklearn.svm import SVR

# Initialize SVR with RBF Kernel
svr = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=.1)

# Train the Model
svr.fit(X_train, y_train)

# Make Predictions
y_pred_svr = svr.predict(X_test)

# Evaluate the Model
mse_svr = mean_squared_error(y_test, y_pred_svr)
print(f"SVR Mean Squared Error: {mse_svr:.2f}")
```

### ğŸ› ï¸ Random Forest Regression Example ğŸ› ï¸

```python
from sklearn.ensemble import RandomForestRegressor

# Initialize Random Forest Regressor
rf = RandomForestRegressor(n_estimators=100, random_state=42)

# Train the Model
rf.fit(X_train, y_train)

# Make Predictions
y_pred_rf = rf.predict(X_test)

# Evaluate the Model
mse_rf = mean_squared_error(y_test, y_pred_rf)
print(f"Random Forest Mean Squared Error: {mse_rf:.2f}")
```

---

## 6. ğŸ“ˆ Model Evaluation for Time Series ğŸ“ˆ

### ğŸ“‰ Mean Absolute Error (MAE) ğŸ“‰

Measures the average magnitude of the errors in a set of predictions, without considering their direction.

```python
from sklearn.metrics import mean_absolute_error

mae = mean_absolute_error(y_test, y_pred)
print(f"Mean Absolute Error: {mae:.2f}")
```

### ğŸ“ Mean Squared Error (MSE) ğŸ“

Measures the average of the squares of the errors, giving higher weight to larger errors.

```python
from sklearn.metrics import mean_squared_error

mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error: {mse:.2f}")
```

### ğŸ“ Root Mean Squared Error (RMSE) ğŸ“

The square root of MSE, representing the standard deviation of the residuals.

```python
import numpy as np

rmse = np.sqrt(mse)
print(f"Root Mean Squared Error: {rmse:.2f}")
```

### ğŸ“ˆ Mean Absolute Percentage Error (MAPE) ğŸ“ˆ

Measures the accuracy as a percentage, useful for understanding the error relative to the actual values.

```python
def mean_absolute_percentage_error(y_true, y_pred): 
    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100

mape = mean_absolute_percentage_error(y_test, y_pred)
print(f"Mean Absolute Percentage Error: {mape:.2f}%")
```

### ğŸ”„ Cross-Validation for Time Series ğŸ”„

Use `TimeSeriesSplit` to perform cross-validation without shuffling the data, preserving the temporal order.

```python
from sklearn.model_selection import TimeSeriesSplit, cross_val_score

# Initialize TimeSeriesSplit
tscv = TimeSeriesSplit(n_splits=5)

# Perform Cross-Validation with Linear Regression
scores = cross_val_score(LinearRegression(), X, y, cv=tscv, scoring='neg_mean_squared_error')
print(f"Cross-Validation MSE Scores: {-scores}")
print(f"Average MSE: {-scores.mean():.2f}")
```

---

## 7. ğŸ› ï¸ğŸ“ˆ Example Project: Sales Forecasting ğŸ› ï¸ğŸ“ˆ

### ğŸ“‹ Project Overview

**Objective**: Develop a machine learning pipeline to forecast daily sales based on historical sales data and related features.

**Tools**: Python, Scikit-Learn, pandas, NumPy, Matplotlib, Seaborn

### ğŸ“ Step-by-Step Guide

#### 1. Load and Explore the Dataset

```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load Sales Dataset
df = pd.read_csv('daily_sales.csv', parse_dates=['Date'])
df.set_index('Date', inplace=True)
print(df.head())

# Plot Sales Over Time
df['Sales'].plot(title='Daily Sales Over Time')
plt.show()

# Check for Seasonality and Trend
sns.lineplot(x=df.index, y='Sales', data=df)
plt.title('Sales Trend')
plt.show()
```

#### 2. Data Preprocessing

```python
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Handle Missing Values if any
df.fillna(method='ffill', inplace=True)

# Create Lag Features
df['Lag1'] = df['Sales'].shift(1)
df['Lag2'] = df['Sales'].shift(2)

# Create Rolling Features
df['Rolling_Mean_7'] = df['Sales'].rolling(window=7).mean()
df['Rolling_STD_7'] = df['Sales'].rolling(window=7).std()

# Create Time-Based Features
df['Day'] = df.index.day
df['Month'] = df.index.month
df['Weekday'] = df.index.weekday

# Drop NaN Values
df.dropna(inplace=True)
print(df.head())
```

#### 3. Feature Engineering

```python
from sklearn.preprocessing import PolynomialFeatures

# Initialize PolynomialFeatures with degree=2
poly = PolynomialFeatures(degree=2, include_bias=False)
poly_features = poly.fit_transform(df[['Lag1', 'Lag2', 'Rolling_Mean_7', 'Rolling_STD_7']])

# Create a DataFrame with polynomial features
poly_df = pd.DataFrame(poly_features, columns=poly.get_feature_names_out())
df = pd.concat([df, poly_df], axis=1)
print(df.head())
```

#### 4. Building Forecasting Models

```python
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR

# Define Features and Target
X = df.drop('Sales', axis=1)
y = df['Sales']

# Split the Data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)

# Initialize Models
lr = LinearRegression()
rf = RandomForestRegressor(n_estimators=100, random_state=42)
svr = SVR(kernel='rbf')

# Train Models
lr.fit(X_train, y_train)
rf.fit(X_train, y_train)
svr.fit(X_train, y_train)
```

#### 5. Evaluating Model Performance

```python
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Make Predictions
y_pred_lr = lr.predict(X_test)
y_pred_rf = rf.predict(X_test)
y_pred_svr = svr.predict(X_test)

# Calculate Metrics
def evaluate_model(y_true, y_pred, model_name):
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100
    print(f"{model_name} Performance:")
    print(f"  MSE: {mse:.2f}")
    print(f"  RMSE: {rmse:.2f}")
    print(f"  MAE: {mae:.2f}")
    print(f"  RÂ²: {r2:.2f}")
    print(f"  MAPE: {mape:.2f}%\n")

evaluate_model(y_test, y_pred_lr, "Linear Regression")
evaluate_model(y_test, y_pred_rf, "Random Forest Regressor")
evaluate_model(y_test, y_pred_svr, "Support Vector Regressor")
```

#### 6. Selecting the Best Model

Based on the evaluation metrics, choose the model with the best performance (e.g., lowest MSE and RMSE, highest RÂ²).

#### 7. Making Future Predictions

```python
# Assume we have new data for the next day
new_data = {
    'Lag1': [y_test.iloc[-1]],
    'Lag2': [y_test.iloc[-2]],
    'Rolling_Mean_7': [df['Rolling_Mean_7'].iloc[-1]],
    'Rolling_STD_7': [df['Rolling_STD_7'].iloc[-1]],
    'Day': [df.index[-1].day + 1],
    'Month': [df.index[-1].month],
    'Weekday': [(df.index[-1].weekday() + 1) % 7]
}

new_df = pd.DataFrame(new_data)

# Create Polynomial Features
new_poly = poly.transform(new_df[['Lag1', 'Lag2', 'Rolling_Mean_7', 'Rolling_STD_7']])
new_poly_df = pd.DataFrame(new_poly, columns=poly.get_feature_names_out())
new_df = pd.concat([new_df, new_poly_df], axis=1)

# Make Prediction with the Best Model (e.g., Random Forest)
best_model = rf
future_prediction = best_model.predict(new_df)
print(f"Future Sales Prediction: {future_prediction[0]:.2f}")
```

### ğŸ“Š Results and Insights

After implementing advanced feature engineering and evaluating multiple models, the **Random Forest Regressor** outperformed the other models with the lowest MSE and highest RÂ² score. This indicates its robustness and ability to capture complex relationships in the sales data, making it the best choice for forecasting future sales.

---

## 8. ğŸš€ğŸ“ Conclusion and Next Steps ğŸš€ğŸ“

Congratulations on completing **Day 9** of "Becoming a Scikit-Learn Boss in 90 Days"! Today, you mastered **Time Series Analysis**, learning how to preprocess time-dependent data, engineer relevant features, implement forecasting models using regression-based approaches, and evaluate their performance. By working through the sales forecasting example project, you gained hands-on experience in predicting future sales based on historical data.

### ğŸ”® Whatâ€™s Next?

- **Day 10: Advanced Model Interpretability**: Understand methods to interpret and explain your machine learning models.
- **Days 11-90: Specialized Topics and Projects**: Engage in specialized topics such as Natural Language Processing, Computer Vision, deep learning integration, and comprehensive projects to solidify your expertise.
- **Continuous Learning**: Explore advanced libraries and tools that complement Scikit-Learn, such as TensorFlow, Keras, and PyTorch for deep learning applications.

### ğŸ“ Tips for Success

- **Practice Regularly**: Apply the concepts through exercises and real-world projects to reinforce your knowledge.
- **Engage with the Community**: Join forums, attend webinars, and collaborate with peers to broaden your perspective and solve challenges together.
- **Stay Curious**: Continuously explore new features and updates in Scikit-Learn and other machine learning libraries.
- **Document Your Work**: Keep a detailed journal of your learning progress and projects to track your growth and facilitate future learning.

Keep up the great work, and stay motivated as you continue your journey to mastering Scikit-Learn and machine learning! ğŸš€ğŸ“š

---

# ğŸ“œ Summary of Day 9 ğŸ“œ

- **ğŸ§  Introduction to Time Series Analysis**: Gained a foundational understanding of time series data and its unique characteristics.
- **ğŸ“Š Time Series Forecasting Techniques**: Explored various forecasting methods including Moving Averages, Exponential Smoothing, AR Models, ARIMA, and Machine Learning approaches like SVR and Random Forest Regression.
- **ğŸ› ï¸ Implementing Time Series Analysis with Scikit-Learn**: Learned how to prepare time series data, engineer relevant features, and build forecasting models using regression-based techniques.
- **ğŸ“ˆ Model Evaluation for Time Series**: Mastered evaluation metrics such as MAE, MSE, RMSE, MAPE, and appropriate cross-validation techniques for time series data.
- **ğŸ› ï¸ğŸ“ˆ Example Project: Sales Forecasting**: Developed a comprehensive sales forecasting pipeline, implemented multiple forecasting models, evaluated their performance, and selected the best model for future predictions.
  
This structured approach ensures that you build a strong foundation in time series analysis, preparing you for more advanced machine learning topics in the upcoming days. Continue experimenting with the provided code examples, and don't hesitate to explore additional resources to deepen your understanding.

**Happy Learning! ğŸ‰**
