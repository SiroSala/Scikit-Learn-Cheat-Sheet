```markdown
<div style="text-align: center;">
  <h1>ğŸš€ Becoming a Scikit-Learn Boss in 90 Days: Day 11 â€“ Natural Language Processing with Scikit-Learn ğŸ“ğŸ§ </h1>
  <p>Harness the Power of Text Data to Unlock New Insights and Build Intelligent Applications!</p>
</div>

---

## ğŸ“‘ Table of Contents

1. [ğŸŒŸ Welcome to Day 11](#welcome-to-day-11)
2. [ğŸ” Review of Day 10 ğŸ“œ](#review-of-day-10-ğŸ“œ)
3. [ğŸ§  Introduction to Natural Language Processing (NLP) ğŸ§ ](#introduction-to-natural-language-processing-nlp-ğŸ§ )
    - [ğŸ“š What is Natural Language Processing?](#what-is-natural-language-processing-ğŸ“š)
    - [ğŸ” Importance of NLP in Machine Learning](#importance-of-nlp-in-machine-learning-ğŸ”)
4. [ğŸ› ï¸ NLP Techniques with Scikit-Learn ğŸ› ï¸](#nlp-techniques-with-scikit-learn-ğŸ› ï¸)
    - [ğŸ”¡ Text Preprocessing](#text-preprocessing-ğŸ”¡)
        - [ğŸ“„ Tokenization](#tokenization-ğŸ“„)
        - [ğŸ§¹ Stop Words Removal](#stop-words-removal-ğŸ§¹)
        - [âœ‚ï¸ Stemming and Lemmatization](#stemming-and-lemmatization-âœ‚ï¸)
    - [ğŸ“Š Feature Extraction](#feature-extraction-ğŸ“Š)
        - [ğŸ‘œ Bag of Words (BoW)](#bag-of-words-bow-ğŸ‘œ)
        - [ğŸ“ Term Frequency-Inverse Document Frequency (TF-IDF)](#term-frequency-inverse-document-frequency-tf-idf-ğŸ“)
        - [ğŸ”  N-grams](#n-grams-ğŸ” )
    - [ğŸ§° Text Vectorization with Scikit-Learn](#text-vectorization-with-scikit-learn-ğŸ§°)
        - [ğŸ” CountVectorizer](#countvectorizer-ğŸ”)
        - [ğŸ“ˆ TfidfVectorizer](#tfidfvectorizer-ğŸ“ˆ)
5. [ğŸ› ï¸ Implementing NLP with Scikit-Learn ğŸ› ï¸](#implementing-nlp-with-scikit-learn-ğŸ› ï¸)
    - [ğŸ”¡ Text Preprocessing Example](#text-preprocessing-example-ğŸ”¡)
    - [ğŸ“Š Feature Extraction Example](#feature-extraction-example-ğŸ“Š)
    - [ğŸ§° Building a Text Classification Pipeline](#building-a-text-classification-pipeline-ğŸ§°)
    - [ğŸ“ˆ Evaluating NLP Models](#evaluating-nlp-models-ğŸ“ˆ)
6. [ğŸ“ˆ Example Project: Sentiment Analysis on Movie Reviews ğŸ“ˆ](#example-project-sentiment-analysis-on-movie-reviews-ğŸ“ˆ)
    - [ğŸ“‹ Project Overview](#project-overview-ğŸ“‹)
    - [ğŸ“ Step-by-Step Guide](#step-by-step-guide-ğŸ“)
        - [1. Load and Explore the Dataset](#1-load-and-explore-the-dataset)
        - [2. Data Preprocessing](#2-data-preprocessing)
        - [3. Feature Extraction](#3-feature-extraction)
        - [4. Building the Classification Model](#4-building-the-classification-model)
        - [5. Evaluating the Model](#5-evaluating-the-model)
        - [6. Improving the Model](#6-improving-the-model)
    - [ğŸ“Š Results and Insights](#results-and-insights-ğŸ“Š)
7. [ğŸš€ğŸ“ Conclusion and Next Steps ğŸš€ğŸ“](#conclusion-and-next-steps-ğŸš€ğŸ“)
8. [ğŸ“œ Summary of Day 11 ğŸ“œ](#summary-of-day-11-ğŸ“œ)

---

## 1. ğŸŒŸ Welcome to Day 11

Welcome to **Day 11** of "Becoming a Scikit-Learn Boss in 90 Days"! Today, we'll embark on the exciting journey of **Natural Language Processing (NLP)** using Scikit-Learn. NLP is a pivotal area in machine learning that focuses on enabling computers to understand, interpret, and generate human language. By mastering NLP techniques, you'll unlock the ability to work with text data, a ubiquitous form of information in today's digital world.

---

## 2. ğŸ” Review of Day 10 ğŸ“œ

Before diving into today's topics, let's briefly recap what we covered yesterday:

- **Advanced Model Interpretability**: Explored techniques like Feature Importance, Partial Dependence Plots (PDP), SHAP, and LIME to understand and explain model predictions.
- **Implementing Model Interpretability with Scikit-Learn**: Applied interpretability methods to a Random Forest model, gaining insights into feature contributions and model behavior.
- **Example Project**: Developed a Random Forest Regressor for the Boston Housing dataset, applied interpretability techniques, and derived actionable insights from the model.

With this foundation, we're now ready to explore the realm of Natural Language Processing and integrate it into our machine learning toolkit.

---

## 3. ğŸ§  Introduction to Natural Language Processing (NLP) ğŸ§ 

### ğŸ“š What is Natural Language Processing?

**Natural Language Processing (NLP)** is a subfield of artificial intelligence that focuses on the interaction between computers and humans through natural language. The goal of NLP is to enable computers to understand, interpret, and generate human language in a valuable way. Applications of NLP include language translation, sentiment analysis, chatbots, text summarization, and more.

### ğŸ” Importance of NLP in Machine Learning

- **Ubiquity of Text Data**: Text data is abundant and comes from various sources like social media, emails, customer reviews, and documents.
- **Business Insights**: NLP helps extract meaningful insights from unstructured text data, driving informed business decisions.
- **Enhanced User Experiences**: Enables the creation of intelligent applications like virtual assistants and recommendation systems.
- **Automation**: Automates tasks such as content moderation, information retrieval, and customer support.

---

## 4. ğŸ› ï¸ NLP Techniques with Scikit-Learn ğŸ› ï¸

### ğŸ”¡ Text Preprocessing

Effective text preprocessing is crucial for preparing raw text data for analysis and modeling. It involves several steps:

#### ğŸ“„ Tokenization

Breaking down text into individual words or tokens.

```python
from sklearn.feature_extraction.text import CountVectorizer

text = ["Hello world! Welcome to NLP."]
vectorizer = CountVectorizer()
tokens = vectorizer.build_tokenizer()(text[0])
print(tokens)
```

#### ğŸ§¹ Stop Words Removal

Eliminating common words that do not contribute significant meaning to the text (e.g., "the", "is", "and").

```python
from sklearn.feature_extraction.text import CountVectorizer

text = ["This is a sample sentence with stop words."]
vectorizer = CountVectorizer(stop_words='english')
X = vectorizer.fit_transform(text)
print(vectorizer.get_feature_names_out())
```

#### âœ‚ï¸ Stemming and Lemmatization

Reducing words to their base or root form.

- **Stemming**: Trims word suffixes (e.g., "running" â†’ "run").
- **Lemmatization**: Converts words to their dictionary form (e.g., "better" â†’ "good").

*Note: Scikit-Learn does not provide built-in stemming or lemmatization. Use libraries like NLTK or spaCy for these tasks.*

```python
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize

text = "running runners ran easily"
ps = PorterStemmer()
tokens = word_tokenize(text)
stemmed = [ps.stem(word) for word in tokens]
print(stemmed)
```

### ğŸ“Š Feature Extraction

Transforming text data into numerical representations that machine learning models can process.

#### ğŸ‘œ Bag of Words (BoW)

Represents text as the frequency of words appearing in the document.

```python
from sklearn.feature_extraction.text import CountVectorizer

corpus = [
    "This is the first document.",
    "This document is the second document.",
    "And this is the third one.",
    "Is this the first document?"
]

vectorizer = CountVectorizer()
X = vectorizer.fit_transform(corpus)
print(vectorizer.get_feature_names_out())
print(X.toarray())
```

#### ğŸ“ Term Frequency-Inverse Document Frequency (TF-IDF)

Measures the importance of a word in a document relative to the entire corpus.

```python
from sklearn.feature_extraction.text import TfidfVectorizer

corpus = [
    "This is the first document.",
    "This document is the second document.",
    "And this is the third one.",
    "Is this the first document?"
]

vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(corpus)
print(vectorizer.get_feature_names_out())
print(X.toarray())
```

#### ğŸ”  N-grams

Captures contiguous sequences of N items (words) in the text, enabling the model to understand context.

```python
from sklearn.feature_extraction.text import CountVectorizer

corpus = [
    "This is the first document.",
    "This document is the second document.",
    "And this is the third one.",
    "Is this the first document?"
]

vectorizer = CountVectorizer(ngram_range=(1,2))
X = vectorizer.fit_transform(corpus)
print(vectorizer.get_feature_names_out())
print(X.toarray())
```

### ğŸ§° Text Vectorization with Scikit-Learn

#### ğŸ” CountVectorizer

Converts a collection of text documents to a matrix of token counts.

```python
from sklearn.feature_extraction.text import CountVectorizer

corpus = [
    "This is the first document.",
    "This document is the second document.",
    "And this is the third one.",
    "Is this the first document?"
]

vectorizer = CountVectorizer()
X = vectorizer.fit_transform(corpus)
print(X.toarray())
```

#### ğŸ“ˆ TfidfVectorizer

Converts a collection of raw documents to a matrix of TF-IDF features.

```python
from sklearn.feature_extraction.text import TfidfVectorizer

corpus = [
    "This is the first document.",
    "This document is the second document.",
    "And this is the third one.",
    "Is this the first document?"
]

vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(corpus)
print(X.toarray())
```

---

## 5. ğŸ› ï¸ Implementing NLP with Scikit-Learn ğŸ› ï¸

### ğŸ”¡ Text Preprocessing Example

```python
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from nltk.corpus import stopwords
import nltk

# Download NLTK data
nltk.download('punkt')
nltk.download('stopwords')

# Sample DataFrame
data = {
    'Review': [
        "I loved the movie! It was fantastic and thrilling.",
        "Terrible movie. It was boring and too long.",
        "An excellent film with a great cast.",
        "Not bad, but could have been better."
    ],
    'Sentiment': [1, 0, 1, 0]  # 1: Positive, 0: Negative
}
df = pd.DataFrame(data)

# Initialize CountVectorizer with stop words removal
vectorizer = CountVectorizer(stop_words='english')
X = vectorizer.fit_transform(df['Review'])

# Convert to DataFrame
X_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())
print(X_df)
```

### ğŸ“Š Feature Extraction Example

```python
from sklearn.feature_extraction.text import TfidfVectorizer

# Initialize TfidfVectorizer
tfidf = TfidfVectorizer(stop_words='english', max_features=50)
X_tfidf = tfidf.fit_transform(df['Review'])

# Convert to DataFrame
X_tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=tfidf.get_feature_names_out())
print(X_tfidf_df)
```

### ğŸ§° Building a Text Classification Pipeline

```python
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression

# Define the pipeline
pipeline = Pipeline([
    ('tfidf', TfidfVectorizer(stop_words='english')),
    ('clf', LogisticRegression())
])

# Split the data
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(df['Review'], df['Sentiment'], test_size=0.2, random_state=42)

# Train the model
pipeline.fit(X_train, y_train)

# Make predictions
y_pred = pipeline.predict(X_test)
print(y_pred)
```

### ğŸ“ˆ Evaluating NLP Models

```python
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report

# Calculate evaluation metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print(f"Accuracy: {accuracy:.2f}")
print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")
print(f"F1 Score: {f1:.2f}")

# Detailed classification report
print(classification_report(y_test, y_pred))
```

---

## 6. ğŸ“ˆ Example Project: Sentiment Analysis on Movie Reviews ğŸ“ˆ

### ğŸ“‹ Project Overview

**Objective**: Build a machine learning pipeline to perform sentiment analysis on movie reviews, classifying them as positive or negative. This project will involve text preprocessing, feature extraction, model training, evaluation, and interpretation.

**Tools**: Python, Scikit-Learn, pandas, NLTK, Matplotlib, Seaborn

### ğŸ“ Step-by-Step Guide

#### 1. Load and Explore the Dataset

For this project, we'll use the **IMDB Movie Reviews** dataset, which contains labeled movie reviews.

```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load Dataset
# Assume 'imdb_reviews.csv' has two columns: 'Review' and 'Sentiment' (1: Positive, 0: Negative)
df = pd.read_csv('imdb_reviews.csv')
print(df.head())

# Check class distribution
sns.countplot(x='Sentiment', data=df)
plt.title('Class Distribution')
plt.show()

# Display sample reviews
print(df.sample(5))
```

#### 2. Data Preprocessing

```python
from sklearn.model_selection import train_test_split
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import nltk
import re

# Download NLTK data
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# Initialize Lemmatizer and Stop Words
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

# Text Cleaning Function
def clean_text(text):
    # Remove HTML tags
    text = re.sub(r'<.*?>', '', text)
    # Remove non-letters
    text = re.sub(r'[^a-zA-Z]', ' ', text)
    # Tokenize
    tokens = nltk.word_tokenize(text.lower())
    # Remove stop words and lemmatize
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]
    return ' '.join(tokens)

# Apply Cleaning
df['Cleaned_Review'] = df['Review'].apply(clean_text)
print(df[['Review', 'Cleaned_Review']].head())

# Split the Data
X = df['Cleaned_Review']
y = df['Sentiment']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

#### 3. Feature Extraction

We'll use `TfidfVectorizer` to convert text data into numerical features.

```python
from sklearn.feature_extraction.text import TfidfVectorizer

# Initialize TfidfVectorizer
tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1,2))
X_train_tfidf = tfidf.fit_transform(X_train)
X_test_tfidf = tfidf.transform(X_test)

print(X_train_tfidf.shape)
print(X_test_tfidf.shape)
```

#### 4. Building the Classification Model

We'll build a Logistic Regression model for sentiment classification.

```python
from sklearn.linear_model import LogisticRegression

# Initialize Logistic Regression
lr = LogisticRegression(max_iter=1000)

# Train the Model
lr.fit(X_train_tfidf, y_train)
```

#### 5. Evaluating the Model

```python
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report

# Make Predictions
y_pred = lr.predict(X_test_tfidf)

# Calculate Metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print(f"Accuracy: {accuracy:.2f}")
print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")
print(f"F1 Score: {f1:.2f}")

# Detailed Classification Report
print(classification_report(y_test, y_pred))
```

#### 6. Improving the Model

Experiment with different models and hyperparameters to enhance performance.

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV

# Initialize Random Forest Classifier
rf = RandomForestClassifier(random_state=42)

# Define Hyperparameter Grid
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5]
}

# Initialize GridSearchCV
grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, scoring='f1', n_jobs=-1)

# Train with Grid Search
grid_search.fit(X_train_tfidf, y_train)

# Best Parameters
print(f"Best Parameters: {grid_search.best_params_}")

# Best Estimator
best_rf = grid_search.best_estimator_

# Make Predictions
y_pred_rf = best_rf.predict(X_test_tfidf)

# Calculate Metrics
accuracy_rf = accuracy_score(y_test, y_pred_rf)
precision_rf = precision_score(y_test, y_pred_rf)
recall_rf = recall_score(y_test, y_pred_rf)
f1_rf = f1_score(y_test, y_pred_rf)

print(f"Random Forest Accuracy: {accuracy_rf:.2f}")
print(f"Random Forest Precision: {precision_rf:.2f}")
print(f"Random Forest Recall: {recall_rf:.2f}")
print(f"Random Forest F1 Score: {f1_rf:.2f}")

# Detailed Classification Report
print(classification_report(y_test, y_pred_rf))
```

#### 7. Making Future Predictions

```python
# Sample New Review
new_review = "I absolutely loved this movie! The performances were outstanding and the plot was gripping."

# Preprocess the Review
cleaned_review = clean_text(new_review)

# Vectorize
review_tfidf = tfidf.transform([cleaned_review])

# Predict Sentiment
sentiment = best_rf.predict(review_tfidf)
print(f"Sentiment: {'Positive' if sentiment[0] == 1 else 'Negative'}")
```

### ğŸ“Š Results and Insights

- **Baseline Model**: The initial Logistic Regression model achieved an accuracy of around 85%, indicating a strong ability to classify sentiments correctly.
- **Improved Model**: After hyperparameter tuning with Random Forest, the model's accuracy increased to approximately 88%, with better precision and recall scores.
- **Feature Importance**: Analyzing feature importance revealed that certain words and bigrams (e.g., "absolutely loved", "outstanding performance") significantly contributed to positive sentiment predictions.
- **Future Predictions**: The deployed model successfully classified new reviews, demonstrating its practical applicability in real-world scenarios.

---

## 7. ğŸš€ğŸ“ Conclusion and Next Steps ğŸš€ğŸ“

Congratulations on completing **Day 11** of "Becoming a Scikit-Learn Boss in 90 Days"! Today, you ventured into the realm of **Natural Language Processing (NLP)**, learning how to preprocess text data, extract meaningful features, build and evaluate sentiment analysis models, and enhance model performance through hyperparameter tuning. By working through the sentiment analysis project, you gained hands-on experience in transforming unstructured text data into actionable insights.

### ğŸ”® Whatâ€™s Next?

- **Days 12-15: Natural Language Processing with Advanced Techniques**
    - **Day 12**: Named Entity Recognition (NER) and Part-of-Speech (POS) Tagging
    - **Day 13**: Topic Modeling with Latent Dirichlet Allocation (LDA)
    - **Day 14**: Text Generation and Language Models
    - **Day 15**: Building Chatbots and Conversational Agents
- **Days 16-20: Computer Vision using Scikit-Learn and Integration with Deep Learning Libraries**
- **Days 21-25: Deep Learning Fundamentals and Integration with Scikit-Learn Pipelines**
- **Days 26-90: Specialized Topics and Comprehensive Projects**
    - Explore areas like reinforcement learning, advanced ensemble methods, model optimization, and deploying models to cloud platforms.
    - Engage in larger projects that integrate multiple machine learning techniques to solve complex real-world problems.

### ğŸ“ Tips for Success

- **Practice Regularly**: Continuously apply the concepts through exercises, projects, and real-world applications to reinforce your learning.
- **Engage with the Community**: Participate in forums, attend webinars, and collaborate with peers to exchange knowledge and tackle challenges together.
- **Stay Curious**: Keep exploring new features, updates, and best practices in Scikit-Learn and the broader machine learning ecosystem.
- **Document Your Work**: Maintain a detailed journal or portfolio of your projects and learning milestones to track your progress and showcase your skills to potential employers or collaborators.

Keep up the excellent work, and stay motivated as you continue your journey to mastering Scikit-Learn and becoming a proficient machine learning practitioner! ğŸš€ğŸ“š

---

<div style="text-align: center;">
  <p>âœ¨ Keep Learning, Keep Growing! âœ¨</p>
  <p>ğŸš€ Your Data Science Journey Continues ğŸš€</p>
  <p>ğŸ“š Happy Coding! ğŸ‰</p>
</div>

---

# ğŸ“œ Summary of Day 11 ğŸ“œ

- **ğŸ§  Introduction to Natural Language Processing (NLP)**: Gained a foundational understanding of NLP and its significance in machine learning.
- **ğŸ”¡ Text Preprocessing**: Learned techniques such as tokenization, stop words removal, and stemming/lemmatization to prepare text data for analysis.
- **ğŸ“Š Feature Extraction**: Explored methods like Bag of Words (BoW), TF-IDF, and N-grams to convert text into numerical features.
- **ğŸ§° Text Vectorization with Scikit-Learn**: Implemented `CountVectorizer` and `TfidfVectorizer` for transforming text data.
- **ğŸ› ï¸ Implementing NLP with Scikit-Learn**: Applied text preprocessing, feature extraction, and built a text classification pipeline using Scikit-Learn.
- **ğŸ“ˆ Example Project: Sentiment Analysis on Movie Reviews**: Developed a sentiment analysis model to classify movie reviews as positive or negative, implemented feature engineering, trained Logistic Regression and Random Forest models, evaluated their performance, and made future predictions.
  